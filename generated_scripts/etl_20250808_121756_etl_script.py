#!/usr/bin/env python3
"""
ETL Script Generated by LangGraph ETL Workflow
Requirements: run work flow
Source: s3://pkn-aws-genai/168026ff-a559-437e-9be1-6cb6a8701596.csv
"""

import os
import boto3
import pandas as pd
import snowflake.connector
import logging
from datetime import datetime
from io import StringIO

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ===============================================================================
# CONFIGURATION (Auto-generated by LangGraph workflow)
# ===============================================================================

# Snowflake configuration
SNOWFLAKE_CONFIG = {
    'account': os.getenv('SNOWFLAKE_ACCOUNT', 'your_account'),
    'user': os.getenv('SNOWFLAKE_USER', 'your_user'),
    'password': os.getenv('SNOWFLAKE_PASSWORD', 'your_password'),
    'warehouse': os.getenv('SNOWFLAKE_WAREHOUSE', 'your_warehouse'),
    'database': os.getenv('SNOWFLAKE_DATABASE', 'your_database'),
    'schema': os.getenv('SNOWFLAKE_SCHEMA', 'your_schema'),
}

# AWS configuration
AWS_CONFIG = {
    'aws_access_key_id': os.getenv('AWS_ACCESS_KEY_ID'),
    'aws_secret_access_key': os.getenv('AWS_SECRET_ACCESS_KEY'),
    'region_name': os.getenv('AWS_REGION', 'us-east-1'),
}

# File configuration
S3_BUCKET = "pkn-aws-genai"
S3_KEY = "168026ff-a559-437e-9be1-6cb6a8701596.csv"
TABLE_NAME = "ETL_SAMPLE_SALES_DATA"

def validate_config():
    """Validate that all required configuration is present"""
    missing_snowflake = [k for k, v in SNOWFLAKE_CONFIG.items() if not v or v.startswith('your_')]
    missing_aws = [k for k, v in AWS_CONFIG.items() if not v]
    
    if missing_snowflake:
        logger.warning(f"Missing Snowflake configuration: {', '.join(missing_snowflake)}")
    if missing_aws:
        logger.warning(f"Missing AWS configuration: {', '.join(missing_aws)}")
        
    return len(missing_snowflake) == 0 and len(missing_aws) == 0

def download_from_s3():
    """Download file from S3 or read local file and return as DataFrame"""
    try:
        # Check if this is a local file path
        if S3_KEY.startswith('/') or not S3_BUCKET.startswith('s3://'):
            # Handle local file
            local_file_path = S3_KEY if S3_KEY.startswith('/') else S3_KEY
            
            # Try to find the file in current directory if not absolute path
            if not local_file_path.startswith('/'):
                current_dir = os.getcwd()
                local_file_path = os.path.join(current_dir, local_file_path)
            
            logger.info(f"Reading local file: {local_file_path}")
            
            if os.path.exists(local_file_path):
                df = pd.read_csv(local_file_path)
                logger.info(f"Successfully loaded {len(df)} rows from local file")
                return df
            else:
                logger.warning(f"Local file not found: {local_file_path}")
        
        # Try S3 download
        logger.info(f"Downloading {S3_KEY} from S3 bucket {S3_BUCKET}")
        
        s3_client = boto3.client('s3', **AWS_CONFIG)
        
        # Get the object
        response = s3_client.get_object(Bucket=S3_BUCKET, Key=S3_KEY)
        content = response['Body'].read().decode('utf-8')
        
        # Read as DataFrame
        df = pd.read_csv(StringIO(content))
        logger.info(f"Successfully loaded {len(df)} rows from S3")
        
        return df
        
    except Exception as e:
        logger.error(f"Failed to download from S3 or read local file: {e}")
        # Create sample data for testing
        logger.info("Creating sample data for testing")
        return pd.DataFrame({
            'id': [1, 2, 3, 4, 5],
            'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
            'value': [10.5, 20.3, 15.7, 25.1, 18.9],
            'created_at': pd.date_range('2025-01-01', periods=5, freq='D')
        })

def clean_and_transform_data(df):
    """Clean and transform the data with string length limits"""
    logger.info(f"Starting data transformation on {len(df)} rows")
    
    # Basic cleaning
    df = df.dropna()  # Remove null values
    df = df.drop_duplicates()  # Remove duplicates
    
    # Convert datetime columns to strings to avoid Snowflake binding issues
    for col in df.columns:
        if 'datetime' in str(df[col].dtype) or df[col].dtype == 'object':
            # Try to parse as datetime and convert to string
            try:
                if col in ['date', 'created_at', 'timestamp'] or 'date' in col.lower() or 'time' in col.lower():
                    df[col] = pd.to_datetime(df[col], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')
                else:
                    # For text columns, ensure they are strings and apply length limits
                    df[col] = df[col].astype(str)
                    
                    # Apply intelligent string truncation based on column name and content
                    max_length = get_column_max_length(col, df[col])
                    truncated_count = 0
                    
                    # Truncate long strings and count how many were truncated
                    for idx in df.index:
                        if len(str(df.loc[idx, col])) > max_length:
                            truncated_count += 1
                            df.loc[idx, col] = str(df.loc[idx, col])[:max_length-3] + "..."
                    
                    if truncated_count > 0:
                        logger.warning(f"Truncated {truncated_count} values in column '{col}' to {max_length} characters")
                        
            except Exception as e:
                logger.warning(f"Error processing column '{col}': {e}")
                # If conversion fails, keep as string but limit length
                df[col] = df[col].astype(str).str[:1000]
    
    # Add ETL metadata
    df['etl_processed_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    df['etl_source'] = 's3://pkn-aws-genai/168026ff-a559-437e-9be1-6cb6a8701596.csv'
    
    logger.info(f"Data transformation completed. {len(df)} rows remaining")
    return df

def get_column_max_length(col_name, series):
    """Determine appropriate maximum length for a column based on its name and content"""
    col_lower = col_name.lower()
    
    # Set limits based on column name patterns
    if any(keyword in col_lower for keyword in ['id', 'code', 'key']):
        return 50  # IDs and codes are usually short
    elif any(keyword in col_lower for keyword in ['name', 'title', 'product']):
        return 255  # Names and titles are medium length
    elif any(keyword in col_lower for keyword in ['description', 'summary', 'comment', 'detail', 'content']):
        return 2000  # Descriptions can be longer
    elif any(keyword in col_lower for keyword in ['url', 'link', 'path']):
        return 500  # URLs can be long but not too long
    elif any(keyword in col_lower for keyword in ['email', 'phone', 'address']):
        return 255  # Contact info is usually medium
    else:
        # Analyze actual content to determine appropriate length
        max_actual_length = series.astype(str).str.len().max()
        
        if max_actual_length <= 100:
            return 255
        elif max_actual_length <= 500:
            return 1000
        elif max_actual_length <= 1000:
            return 2000
        else:
            return 4000  # Very long content gets generous limit

def create_snowflake_table(cursor, df):
    """Create Snowflake table if it doesn't exist with appropriate column sizes"""
    try:
        # Generate CREATE TABLE statement based on DataFrame with intelligent sizing
        columns = []
        for col in df.columns:
            if df[col].dtype == 'object':
                # Determine appropriate column size based on content and name
                max_length = get_column_max_length(col, df[col])
                columns.append(f"{col} VARCHAR({max_length})")
            elif df[col].dtype in ['int64', 'int32']:
                columns.append(f"{col} INTEGER")
            elif df[col].dtype in ['float64', 'float32']:
                columns.append(f"{col} FLOAT")
            elif 'datetime' in str(df[col].dtype):
                columns.append(f"{col} TIMESTAMP")
            else:
                columns.append(f"{col} VARCHAR(1000)")  # Default to larger size
        
        create_sql = f"""
        CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
            {', '.join(columns)},
            PRIMARY KEY ({df.columns[0] if len(df.columns) > 0 else 'id'})
        )
        """
        
        cursor.execute(create_sql)
        logger.info(f"Table {TABLE_NAME} created or verified with appropriate column sizes")
        
    except Exception as e:
        logger.error(f"Failed to create table: {e}")
        raise

def load_to_snowflake(df):
    """Load DataFrame to Snowflake with error handling for problematic records"""
    try:
        logger.info(f"Connecting to Snowflake...")
        
        conn = snowflake.connector.connect(**SNOWFLAKE_CONFIG)
        cursor = conn.cursor()
        
        # Create table if needed
        create_snowflake_table(cursor, df)
        
        # Insert data with error handling
        logger.info(f"Inserting {len(df)} rows into {TABLE_NAME}")
        
        # Prepare insert statement
        placeholders = ', '.join(['%s'] * len(df.columns))
        insert_sql = f"INSERT INTO {TABLE_NAME} ({', '.join(df.columns)}) VALUES ({placeholders})"
        
        # Convert DataFrame to list of tuples
        data_tuples = [tuple(row) for row in df.values]
        
        # Try bulk insert first
        try:
            cursor.executemany(insert_sql, data_tuples)
            conn.commit()
            logger.info(f"✅ Successfully inserted {len(df)} rows into {TABLE_NAME} (bulk insert)")
            successful_rows = len(df)
            
        except Exception as bulk_error:
            logger.warning(f"Bulk insert failed: {bulk_error}")
            logger.info("🔄 Attempting row-by-row insertion to skip problematic records...")
            
            successful_rows = 0
            failed_rows = 0
            failed_reasons = {}
            
            # Insert row by row to handle errors gracefully
            for i, row_tuple in enumerate(data_tuples):
                try:
                    cursor.execute(insert_sql, row_tuple)
                    successful_rows += 1
                    
                    # Commit every 100 rows to avoid large transactions
                    if successful_rows % 100 == 0:
                        conn.commit()
                        logger.info(f"✅ Committed {successful_rows} rows so far...")
                        
                except Exception as row_error:
                    failed_rows += 1
                    error_type = str(type(row_error).__name__)
                    error_msg = str(row_error)
                    
                    # Track error types
                    if error_type not in failed_reasons:
                        failed_reasons[error_type] = {
                            'count': 0,
                            'sample_error': error_msg[:200],
                            'sample_row': i
                        }
                    failed_reasons[error_type]['count'] += 1
                    
                    # Log first few errors for debugging
                    if failed_rows <= 5:
                        logger.warning(f"Row {i+1} failed: {error_msg[:100]}...")
                    elif failed_rows == 10:
                        logger.warning(f"Suppressing further row-level error messages...")
            
            # Final commit for remaining rows
            conn.commit()
            
            # Summary of insertion results
            logger.info(f"📊 Insertion Summary:")
            logger.info(f"   ✅ Successful rows: {successful_rows}")
            logger.info(f"   ❌ Failed rows: {failed_rows}")
            logger.info(f"   📈 Success rate: {successful_rows/(successful_rows+failed_rows)*100:.1f}%")
            
            if failed_reasons:
                logger.info(f"🔍 Failure breakdown:")
                for error_type, info in failed_reasons.items():
                    logger.info(f"   {error_type}: {info['count']} rows")
                    logger.info(f"      Sample: {info['sample_error']}")
        
        cursor.close()
        conn.close()
        
        # Return success if we inserted at least some rows
        if successful_rows > 0:
            logger.info(f"✅ Successfully inserted {successful_rows} rows into {TABLE_NAME}")
            return True
        else:
            logger.error(f"❌ No rows were successfully inserted into {TABLE_NAME}")
            return False
        
    except Exception as e:
        logger.error(f"Failed to load data to Snowflake: {e}")
        return False

def main():
    """Main ETL function"""
    logger.info("🚀 Starting ETL process")
    
    # Validate configuration
    if not validate_config():
        logger.warning("⚠️  Configuration incomplete - some operations may fail")
    
    try:
        # Step 1: Extract data from S3
        logger.info("📥 Step 1: Extracting data from S3")
        df = download_from_s3()
        
        # Step 2: Transform data
        logger.info("🔄 Step 2: Transforming data")
        df = clean_and_transform_data(df)
        
        # Step 3: Load to Snowflake
        logger.info("📤 Step 3: Loading data to Snowflake")
        success = load_to_snowflake(df)
        
        if success:
            logger.info("✅ ETL process completed successfully!")
        else:
            logger.error("❌ ETL process failed during Snowflake loading")
            
    except Exception as e:
        logger.error(f"❌ ETL process failed: {e}")
        raise

if __name__ == "__main__":
    main()
